{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dce2c9-3a4b-4977-90ff-0d1223b6de59",
   "metadata": {},
   "source": [
    "# Week 7: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b928b-3cb5-4327-ad36-8b4e21cec3ca",
   "metadata": {},
   "source": [
    "Last of supervised learning algos in this course.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa978f-2f7f-4f1c-affd-ab4fad698fbd",
   "metadata": {},
   "source": [
    "## Optimization Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2f00-d212-4c35-befb-2bce1ceb72af",
   "metadata": {},
   "source": [
    "Recall logistic reg:\n",
    "\n",
    "$$h_\\theta(x) = \\frac{1}{1 + exp(-\\theta^Tx)}$$\n",
    "\n",
    "If $y= 1$, we want $h_\\theta(x) \\approx 1$ means $\\theta^Tx >> 0$.    \n",
    "\n",
    "If $y= 0$, we want $h_\\theta(x) \\approx 1$ means $\\theta^Tx << 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42c39b-51d3-4f60-b7ba-279ad3f22fba",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig1.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2b904-dbc8-4663-9ec6-4beaefea40b6",
   "metadata": {},
   "source": [
    "### Cost Func for Log regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ceaa9-8724-4d0c-84d2-2e0d1a09867c",
   "metadata": {},
   "source": [
    "One example cost: $-(y \\log h_\\theta(x) + (1-y) \\log (1 - h_\\theta(x)) $\n",
    "$$ = -(y \\log \\frac{1}{1 + exp(-\\theta^Tx)} + (1-y) \\log (1 - \\frac{1}{1 + exp(-\\theta^Tx)})$$  \n",
    "\n",
    "If $y=1$ then want  $\\theta^Tx >> 0$ so that $-\\log \\frac{1}{1 + exp(-\\theta^Tx)} \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af38235-946f-4cf9-821f-d4f86dc00226",
   "metadata": {},
   "source": [
    "In SVM we are going to take the above cost function (term 1), and modify a bit with an elbow approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2031ef-5d93-40c5-80eb-c4b3ba4379b9",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig2.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c0e4a-c7e6-4474-8bfb-3ebbd923d9df",
   "metadata": {},
   "source": [
    "### Cost Function in Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756564a-6fa1-4006-8a72-d3543b371288",
   "metadata": {},
   "source": [
    "**Logistic Regression:**\n",
    "$$\\min _{\\theta} \\frac{1}{m}\\left[\\sum_{i=1}^{m}\\left[ y^{(i)}\\left(-\\log h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right)\\left(-\\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right)\\right]+\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27414-daea-426f-ae1d-58db2e7b4b03",
   "metadata": {},
   "source": [
    "**Support Vector Machine:**\n",
    "\n",
    "$$\\min _{\\theta}\\left[ c \\sum_{i=1}^{m} \\left[y^{(i)}cost_1(\\theta^T x)+\\left(1-y^{(i)}\\right)cost_0(\\theta^T x)\\right]+\\frac{1}{2} \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ac86c-9b79-41a6-9d32-ca4cd1ddb2e8",
   "metadata": {},
   "source": [
    "### SVM Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f310b-2d92-4a87-9c80-505d82551ac7",
   "metadata": {},
   "source": [
    "$$\n",
    "h_\\theta(x) = \\left\\{\\begin{array}{ll}\n",
    "1 & \\text{ if } \\theta^T x \\geq 0 \\\\\n",
    "0 & \\text{ otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7cfeb-7bbe-4319-aea5-f073d0477086",
   "metadata": {},
   "source": [
    "## Large Margin Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763635e-d953-4cc7-a180-1e9e64f3c55d",
   "metadata": {},
   "source": [
    "Think of the cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd013e3a-9d32-4345-abdb-ea4580500c45",
   "metadata": {},
   "source": [
    "$$\\min _{\\theta}\\left[ c \\sum_{i=1}^{m} \\left[y^{(i)}cost_1(\\theta^T x)+\\left(1-y^{(i)}\\right)cost_0(\\theta^T x)\\right]+\\frac{1}{2} \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c80117-1263-4970-818c-eff424811a56",
   "metadata": {},
   "source": [
    "What makes this func small?  \n",
    "For $y = 1$ want $\\theta^T x \\geq 1 \\rightarrow cost_1 = 0$  \n",
    "For $y = 0$ want $\\theta^T x \\leq -1 \\rightarrow cost_0 = 0$  \n",
    "\n",
    "It's not enough to just barely be greater than (less than) 0! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78b97c-c5ce-4a30-a670-c65fd599d223",
   "metadata": {},
   "source": [
    "### Intuition about the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f37a8-8ff8-46db-bc83-aff2a4e0b64e",
   "metadata": {},
   "source": [
    "Suppose $C$ is large, so the first term is more impactful in the objective function. We have a lot of motivation to minimize this as much as possible.\n",
    "\n",
    "Whenever $y^{(i)} = 1:$  need $\\theta^T x \\geq 1$\n",
    "\n",
    "Whenever $y^{(i)} = 0:$  need $\\theta^T x \\leq -1$   \n",
    "\n",
    "We are left so that the first term is 0:\n",
    "\n",
    "$\\min c*0 + 1/2 \\sum \\theta_i^2 $  \n",
    "\n",
    "such that.  the conditions above are in effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6067d6-2b4d-4206-903f-7e3045ba6cc0",
   "metadata": {},
   "source": [
    "This gives interesting decision boundaries. The support vector machines will give the black line. The connection between this and the previous knowledge is unclear atm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a1358-cd85-4106-ac19-11b47f2cdaf2",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig3.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d716b-9dac-411a-9047-ee712c7c98d1",
   "metadata": {},
   "source": [
    "So you might get two decision boundaries. What is the role of $c$? It will give you a black line, which is the large margin. \n",
    "\n",
    "Large margins are very sensitive to outliers. You may end up with the magenta decision boundary, if $c$ is very large!  \n",
    "\n",
    "If $c$ is not too large, you maintain the black boundary, even in the precense of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1d477-682b-4b10-8b80-92ec60b8ec37",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig4.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bec54e-eced-44c3-be52-52f771901b9a",
   "metadata": {},
   "source": [
    "### Mathematics Behind Large Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6e22b-ccbb-427e-9aad-bc91071ed746",
   "metadata": {},
   "source": [
    "For illustration, ignore $\\theta_0$ and define $n = 2$, only 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea90966-59d6-4bfc-9cf2-8ab8124417b2",
   "metadata": {},
   "source": [
    "Consider $\\min_\\theta \\frac{1}{2} \\sum_{j=1}^n \\theta_j^2$  observe:\n",
    "\n",
    "$$\\sum_{j=1}^n \\theta_j^2 = ||\n",
    "\\theta||$$  \n",
    "\n",
    "Remember don't need the other terms in the objective function, bc these constraints force those to be nearly 0! (We also have c to be large enough that what's writte above is what the optimizers would mimick.\n",
    "\n",
    "where we ignore $\\theta_0$ in the calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4726e3b-cdec-49e8-bdba-3754535784cf",
   "metadata": {},
   "source": [
    "No consider: \n",
    "\n",
    "$$\\theta^T x \\geq 1$$  \n",
    "and  \n",
    "$$ \\theta^T x \\leq -1$$\n",
    "\n",
    "and think of: $u^T := \\theta^T, \\ v := x$\n",
    "\n",
    "So if you plot on the $x_1^{(i)}, x_2^{(i)}$ axis, we can see this thing is actually an inner product.  \n",
    "\n",
    "This is like saying \n",
    "$$\\theta^T x^{(i)} \\text{ is the projection of } x \\text{ onto } \\theta$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\theta^T x^{(i)} = p^{(i)} ||\\theta|| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e3c16-7151-49f5-b0e6-a77f35a0faf9",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc5756-2fc0-4d3d-b0d7-649fcbddadea",
   "metadata": {},
   "source": [
    "We can rewrite the previous as:\n",
    "\n",
    "$$p^{(i)} ||\\theta||  \\geq 1$$  \n",
    "and  \n",
    "$$ p^{(i)} ||\\theta|| \\leq -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c3ef9-181e-4704-ae60-e1c1717f3340",
   "metadata": {},
   "source": [
    "Suppose w are trying to make some classification and determine the decision boundary. Look at the green, it is a small margin decision boundary. Parameter vector $\\theta$ is orthogonal to the decision boundary.   \n",
    "\n",
    "What implies: $p^{(1)}$ and $p^{(2)}$ are small, so for the above constraits to be respected, we must have $||\\theta||$ be large to compenste. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87daab52-aa25-499e-a4ce-07948fa6dfd5",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig7.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6385cf-b322-4aaf-8a53-62a654805c55",
   "metadata": {},
   "source": [
    "**In constrast** look at this other decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19130a36-d163-40d3-bd21-4db4c2f9b6b5",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig8.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540564bc-8fcf-4fec-b779-1221317bd145",
   "metadata": {},
   "source": [
    "It has $p^{(1,2)}$ as much larger, ie it has chosen $\\theta$ so the projections are incredibly aliged with $\\theta$. This means the value for $p$'s are must smaller. To maintain the previous constraits $p^{(i)} ||\\theta|| \\geq 1$ , $||\\theta||$ can be smaller, which is what the objective function is trying to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b793fba-3009-4d56-afae-7142a967005e",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig9.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bb2d7-a758-45dd-9e45-c026674bc3d0",
   "metadata": {},
   "source": [
    "  We see that these $p^{(i)}$s are larger, and they are the ones that determine the margine's size. The tell you how far from the boundary you are!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11efd0c-b0da-41cc-afe3-e361fe3d1e8b",
   "metadata": {},
   "source": [
    "## Kernels I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b2a3-ca24-4e72-9c17-10bcc9e66a20",
   "metadata": {},
   "source": [
    "Kernels are main teqn for nonlinear decision boundaries.  \n",
    "\n",
    "Want complex high order polynomial feats. Train feats as if they're independent.   \n",
    "\n",
    "Is there a better way to do this below?  \n",
    "\n",
    "We know that more complexity means more expensive compu time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e712d7-9541-4660-bcdc-1e2320df361e",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig10.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c438cd-e835-47ec-b3b4-9afc80e765fc",
   "metadata": {},
   "source": [
    "### Kernels To Solve The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af437f6-20a7-402e-8030-4dfbf1c866fe",
   "metadata": {},
   "source": [
    "Given $x$, compute new features depending on proximity to landmarks $l^{(i)}$s.  \n",
    "\n",
    "Compute $f_i = similarity(x, l^{(i)}) = exp\\left(-\\frac{||x - l^{(i)}||^2}{2 \\sigma^2} \\right)$.  \n",
    "\n",
    "**Similarity function** is called **Kernel** function.  \n",
    "$$ K(x, l^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa44eb-4b5e-466c-a78f-3e533d0b18b3",
   "metadata": {},
   "source": [
    "**What happens with these kernels?**  \n",
    "\n",
    "- If $x \\approx l^{(i)}:$ \n",
    "    - $f_i \\approx exp\\left(-\\frac{||x - l^{(i)}||^2}{2 \\sigma^2} \\right) \\approx exp\\left(-\\frac{0}{2 \\sigma^2} \\right) \\approx 1$\n",
    "\n",
    "- If $x$ far from $l^{(i)}:$ \n",
    "    - $f_i \\approx exp\\left(-\\frac{||x - l^{(i)}||^2}{2 \\sigma^2} \\right) \\approx exp\\left(-\\frac{\\infty}{2 \\sigma^2} \\right) \\approx 0$\n",
    "    \n",
    "    \n",
    "- each of the landmarks defines a new featur $l^{(i)} \\rightarrow f_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d239e2-0163-4572-96ae-215670621fb4",
   "metadata": {},
   "source": [
    "#### Here Is An Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bfd55-ad32-44f7-8063-b5f7ba5dd1e7",
   "metadata": {},
   "source": [
    "Suppose $l^{(1)} = [3, 5]$, $f_1 =  exp\\left(-\\frac{||x - l^{(1)}||^2}{2 \\sigma^2} \\right)$, and $\\sigma^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f78fb-ecd9-40d9-9006-48ced7c48443",
   "metadata": {},
   "source": [
    "Given training example $x$, compute $f_1, f_2, f_3$ and predict \"1\" when \n",
    "\n",
    "$$ \\theta_0 + \\theta_1f_1 + \\theta_2 f_2 + \\theta_3 f_3,$$\n",
    "\n",
    "with $\\theta_0 = -0.5$, $\\theta_1 = 1$, and $\\theta_2 = 1$, and $\\theta_3 = 0$. \n",
    "\n",
    "This gives:  \n",
    "\n",
    "$\\theta_0 + \\theta_1 * 1 + \\theta_2 *0 + \\theta_3 * 0 = -0.5 + 1 \\geq 0.$ which predicts $y =1.$  \n",
    "\n",
    "This is for the magenta x.  \n",
    "What about for cyan x?\n",
    "\n",
    "$f_0 , f_1, f_2 \\approx 0$, so we get  \n",
    "\n",
    "$\\theta_0 + \\theta_1 * 0 + \\theta_2 *0 + \\theta_3 * 0 = -0.5  \\leq 0.$ which predicts $y = 0.$  \n",
    "\n",
    "Based on this $\\theta$ values, we see any points not close enough to $l^1, l^2$ will result in value of $y=0$, creating this given decision boundary!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5a7cb-72a1-4cea-92a9-1623b4a021ae",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig11.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c2f74-67bd-486a-b544-c5d3473098d4",
   "metadata": {},
   "source": [
    "## Kernels II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940168ae-50c5-4be9-8c9b-b258bd29b83a",
   "metadata": {},
   "source": [
    "Where to get $l^{(i)}$s?. \n",
    "\n",
    "Predict $y=1$ if $\\theta_0 + \\theta_i*f_i ... \\geq 0$.  \n",
    "\n",
    "In practice, suppose we have positiv and neg examples.  \n",
    "\n",
    "For each train example we have, we will put landmarks as exact locations of training examples.  \n",
    "\n",
    "That is define $l^{(i)} := x^{(i)}$.  \n",
    "\n",
    "We will end up with $l^{(i)}$ for $i=1\\ldots m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8a10b-a240-4eae-84b5-cacd6300b3a1",
   "metadata": {},
   "source": [
    "Now compute features: $f_i = similarity(x, l^{(i)}).$   \n",
    "\n",
    "Then group the feature vector $f = [f_0, \\ldots f_m]$   \n",
    "\n",
    "We can now represent $x^{(i)}$ as a feature vector $[f^{(i)}]$ as the way to rep $x$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ed9b-415c-4e2a-b5c1-066c44c6172d",
   "metadata": {},
   "source": [
    "### SVM With Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4dc17-5b28-4131-915e-37f37731ff17",
   "metadata": {},
   "source": [
    "Hypothesis: given $x$ compute features $f \\in R^{m+1}$. \n",
    "\n",
    "Predict $y = 1$ if $\\theta^T f \\geq 0$.   \n",
    "\n",
    "For Training:  \n",
    "\n",
    "$$\\min_\\theta C \\sum_{i=1}^m y^{(i)} cost_1(\\theta^T fT{(i)}) +  (1-y^{(i)}) cost_0(\\theta^T f^{(i)}) + \\frac{1}{2} \\sum_{j=1}^n \\theta_j^2 $$  \n",
    "\n",
    "And the effective number of features is wrt to $f$, which has $m+1$ features (the +1 is for the interecept). Remember, for each $x$ there is an $f$ and there are $m$ $x's!$  \n",
    "\n",
    "This last piece can be thought of as a deviation form the original $\\theta^T\\theta,$ because we don't just have $n$ points and now have $m$ points.  \n",
    "\n",
    "$$\\theta^T \\theta \\rightarrow \\theta^T M \\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ce127-7f13-4889-90e1-9596796c0d1b",
   "metadata": {},
   "source": [
    "### SVM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa919190-3f6e-4969-8d67-58a32f7d30f0",
   "metadata": {},
   "source": [
    "$C = (1/ \\lambda)$ \n",
    "- Large $C$: low bias, hi var\n",
    "- Small $C$: hi bias, low var  \n",
    "\n",
    "$\\sigma^2$\n",
    "- Large $\\sigma^2:$ features $f_i$, vary smooothly.\n",
    "    - hi bi, lo var\n",
    "- Small $\\sigma^2:$ Features $f_i$, vary not smoohtly.\n",
    "    - lower bi, hi var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1c125-74b7-4ecd-9ed6-82eedad640db",
   "metadata": {},
   "source": [
    "<img src = \"figures/fig12.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fef3a-f8ac-401b-b1b0-8fdf68d02e0f",
   "metadata": {},
   "source": [
    "## Using An SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf9a6a-023b-4ea5-b860-e9a76fc4de78",
   "metadata": {},
   "source": [
    "Not recommended to write your own SVM software package to solve for parameters $\\theta$. Use libs like (liblinear, libsvm, ...). Many others have written long-standing/tested software libraries to solve many of the math guts of your problem. You should not be writing your own software, but there are other parts of the problem that you will need to be hands-on with.   \n",
    "\n",
    "Need to specify:\n",
    "1. Choice of C\n",
    "2. Choice of kernel (similarity function)  \n",
    "\n",
    "Note, could specify the no kernel (linear kernel), which is  \n",
    "$y = 1$ if $\\theta^Tx\\geq0$.\n",
    "\n",
    "Or specify params of chosen kernel ex:  \n",
    "Gaussin kernel, choose $\\sigma^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547360ef",
   "metadata": {},
   "source": [
    "### Gaussian Kernel As Similarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e4a02",
   "metadata": {},
   "source": [
    "$$f = exp\\left(- \\frac{|| x1 - x2||^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca1ce7",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "1. $x1 = x^{(i)}$ and $x2 = f^{(i)}$\n",
    "2. Do perform feature scaling before using the Gaussian Kernel.\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302a040",
   "metadata": {},
   "source": [
    "### Other Kernel Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1448d",
   "metadata": {},
   "source": [
    "Not all similarity functions make valid kernels.  \n",
    "Kernels must satisfy **Mercer's Theorem** to be sure SVM package optimizations run correct and do not diverge!  \n",
    "\n",
    "https://en.wikipedia.org/wiki/Mercer%27s_theorem  \n",
    "\n",
    "- polynomial Kernel: $k(x, l) = (x^T l)^2, \\ (x^T l)^2, \\ (x^T l + 1)^3 \\ldots (x^T l + const)^{degree}$ \n",
    "    - generally perform worse than Gaussian\n",
    "    \n",
    "- String kernel (input data is strings), chi-sq Kernel, histogram intersection kernel, ...\n",
    "    - ex: may want similarity between two strings, $l, x$, then use string kernel: $string_sim(l,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49c99a",
   "metadata": {},
   "source": [
    "## Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92aea7",
   "metadata": {},
   "source": [
    "Find approp decis boundary with SVM.  \n",
    "\n",
    "For $k$ classes use $k$ svms! \n",
    "\n",
    "train k svms, one to distinguish $y=i$ from the rest.\n",
    "\n",
    "Yields vectors $\\theta^{(i)}, i = 1\\ldots k$, one for each class.  \n",
    "\n",
    "pick class $i$ with largest ${\\theta^{(i)}}^Tx$ ie\n",
    "\n",
    "$$prediction = \\arg \\max_{{i}} {\\theta^{(i)}}^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45430945",
   "metadata": {},
   "source": [
    "### Logistic Reg Vs SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c5fa3",
   "metadata": {},
   "source": [
    "$n = $features no. $x \\in R^{n+1}$  \n",
    "$m = $number train samples.  \n",
    "\n",
    "If $n>>m$ \n",
    "- $n$ large relative to $m$, \n",
    "- $n=10k$, $m =10...1000$  \n",
    "- use logistic regression or SVM without a kernel (\"linear kernel\"). This means there is not enough data to fit a more complicated non-linear kernel/model.  \n",
    "\n",
    "If $n$ small, $m$ intermediate \n",
    "- $n=1-1000k$, $m =10- 10000$ \n",
    "- use SVM with Gaussian kernel \n",
    "\n",
    "If $n$ small, $m$ large \n",
    "- $n=1-1000k$, $m =50k+$ \n",
    "- create/add more feats, then use logistic reg or no-kernel SVM\n",
    "\n",
    "Neural networks likely to work well for most of these settings, but may be slower to train, esepcially when $n$ small, $m$ intermediate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d018520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
