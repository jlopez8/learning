{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51d8d25-e1d6-4c38-8894-d4edc41fc405",
   "metadata": {},
   "source": [
    "# Week 8: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22fb27-b174-4b18-a7bf-b391f9430037",
   "metadata": {},
   "source": [
    "Assume we have feature scaled and mean normalized some data.  \n",
    "\n",
    "We could project our data for dimensionality reduction on many planes/lines.   \n",
    "\n",
    "PCA would tell you which line to choose.  \n",
    "\n",
    "**Goal of PCA** Find direction (vector $u^{(1)} \\in R^n$) onto which to project the data so as to minimize the projection error.  \n",
    "\n",
    "This case shows 2 dim to 1 dim.  Generally: \n",
    "**Reduce n-dim to k-dim** by finding $k$ vectors $u^{(1)} \\ldots u^{(k)}$ so as to minimize projection errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aae0bc-65c1-41c7-bde8-b83e98c23aea",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig18.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8f34b-6711-4779-914d-86059e072cfb",
   "metadata": {},
   "source": [
    "### Example:  $R^3 \\to R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776864a-57bc-47ec-b404-2905c7e0d15c",
   "metadata": {},
   "source": [
    "Find 2 vectors so that the plan defined by them minimizes the projection distance using the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae22fc5-88c9-4c47-82c5-5b29790d5924",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig19.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58372c0-b92c-460c-b840-81598a304ab1",
   "metadata": {},
   "source": [
    "## PCA Is Not Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2bfb8-3468-4239-bb62-37c55bcfa3b8",
   "metadata": {},
   "source": [
    "**Lin Reg:** Find straight line so to minimize squared error between line and the points.  \n",
    "- tries to use values of x to predict y.\n",
    "\n",
    "**PCA:** Tries to minimize the shortest orthogonal distance between points and the line.\n",
    "- there is no special variable y I'm trying to predict. Just have multiple features treated equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2caa255-0028-4fd2-a842-2a055c578c9a",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig20.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2577e-118e-4be6-99d1-0a952b0906f3",
   "metadata": {},
   "source": [
    "## Principle Component Analysis Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bf28d-f34e-41e7-9ece-66214878a721",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43733943-cf65-4906-9ffc-110c9c6ee942",
   "metadata": {},
   "source": [
    "Train set: $x^{(i)}, \\  i = 1 \\ldots m$   \n",
    "\n",
    "Now mean normalize:  \n",
    "\n",
    "$\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}$  \n",
    "\n",
    "$$x^{(i)} \\leftarrow x^{(i)}-\\mu_j$$\n",
    "\n",
    "For different featurs of different values, then normalize to stan dev.:  \n",
    "\n",
    "$$x^{(i)} \\leftarrow \\frac{x^{(i)}-\\mu_j}{s_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738ddc1-0ab4-475c-9be4-92b8dbce1049",
   "metadata": {},
   "source": [
    "### Compute Covariance Matrix $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38169af-55b2-49b0-8e00-4aa2317e7333",
   "metadata": {},
   "source": [
    "$$\\Sigma = \\frac{1}{m} \\sum_1^n (x^{(i)})(x^{(i)})^T $$\n",
    "\n",
    "- $\\Sigma$ always satisfy symmetric positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da0900-5c93-41f3-a9a3-48675041a554",
   "metadata": {},
   "source": [
    "### Compute Singular Value Decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca480278-61f2-427d-9a31-610759e57b24",
   "metadata": {},
   "source": [
    "$$\\Sigma = U S V^T$$  \n",
    "\n",
    "The col vecs $U$ are the ones you want for the PCA. Just take the first $k$ of them. \n",
    "$$U = [u^{(1)}, u^{(2)}, \\ldots, u^{(m)}] \\in R^{n\\times n}$$  \n",
    "\n",
    "$$U_{reduce} = [u^{(1)}, u^{(2)}, \\ldots, u^{(k)}] \\in R^{n\\times k}$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129a42c-ba69-4897-9025-83f76705a0fb",
   "metadata": {},
   "source": [
    "### Compute z The Reduced Data Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac2315-d9e8-4bcf-99c2-4ff83772c0ac",
   "metadata": {},
   "source": [
    "Then form: $z$, the reduced data vector, using $U_{reduce}$ data vector $x \\in R^{n\\times 1}$  \n",
    "\n",
    "$$z = U_{reduce}^T x \\in R^{k \\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa89871-0351-4211-8c48-d33f0788b5b3",
   "metadata": {},
   "source": [
    "**NOTE: DO NOT SET x_0 = 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dee6e-2f6c-4be1-a283-6e6fa89c3693",
   "metadata": {},
   "source": [
    "## Applying PCA: Reconstruction from Compressed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267000b9-0aeb-4f46-8022-cf20c43bb896",
   "metadata": {},
   "source": [
    "Now that we have a reduced data vector, how can we go back to the original space?  \n",
    "\n",
    "$$ z \\in R^k \\to x \\in R^n, \\ k<n$$ \n",
    "\n",
    "$$ z = U^T_{reduce} x $$\n",
    "\n",
    "Fact is, we will end up with an **approximation** to the original data: \n",
    "\n",
    "$$ x \\approx x_{approx} = U_{reduce} z $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5405c-c2fd-49b2-9cf2-4971dd6c7803",
   "metadata": {},
   "source": [
    "**Note:** $U_{reduce}$ is a unitary matrix, because $U$ is unitary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138bd19-abe1-42eb-b9bb-f3a0e186bb59",
   "metadata": {},
   "source": [
    "## Applying PCA: ChoosingThe Number of Principle Components in PCA $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd673fcb-9ff1-443c-b05a-bf5c5d883f8a",
   "metadata": {},
   "source": [
    "**Average squared projection error:**\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}-x_{\\text {approx }}^{(i)}\\right\\|^{2}\n",
    " \\end{align}\n",
    " \n",
    "**Total variation in data:** \n",
    "\\begin{align} \n",
    "\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}\\right\\|^{2}\n",
    "\\end{align}\n",
    "\n",
    "**Rule of thumb 99% of varaince is retained**\n",
    "\n",
    "Choose $k$ = smallest value so that:\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}-x_{\\text {approx }}^{(i)}\\right\\|^{2}}{\n",
    "\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}\\right\\|^{2}} \\leq 0.01 \\ \\ (1\\%)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6d2ba-4462-4f95-9ada-74fd2f555e61",
   "metadata": {},
   "source": [
    "Other common values:  \n",
    "- 0.05 (95% retained)\n",
    "- 0.10 (90% retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783a77b-5624-4876-b2eb-0762c12c3744",
   "metadata": {},
   "source": [
    "**Notes:** Turns out much real life data is highly correlated so you can reduce many of the features while retaining much of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2578a20-a673-4bc1-92a7-9995bec14d2e",
   "metadata": {},
   "source": [
    "## Implementing Choosing $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16d446-69fc-45f4-94cb-b7a897fdc41b",
   "metadata": {},
   "source": [
    "1. Try with $k=1$\n",
    "2. Compute $U_{reduce}, \\ z^{(1)}, ...z^{(k)}$\n",
    "3. Check if: \n",
    "    $$\\begin{align}\n",
    " \\frac{\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}-x_{\\text {approx }}^{(i)}\\right\\|^{2}}{\n",
    "\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}\\right\\|^{2}} \\leq 0.01 \\ \\ (1\\%)\n",
    "\\end{align}$$\n",
    "4. If not, then increase $k$. Repeat until you reach threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215d347-fe19-4efe-bfea-ee22847b12f7",
   "metadata": {},
   "source": [
    "**More or less horribly inefficient**\n",
    "\n",
    "You can do this instead ->  **MUCH MORE EFFICIENT!**\n",
    "\n",
    "When calling SVD, you get $U, S, V$. \n",
    "\n",
    "Matrix $S$ is diagonal , it contains the singular values.  \n",
    "These values help to compute exactly the test quantity above so that:\n",
    "\n",
    "\\begin{align}\n",
    "%\n",
    "\\frac{\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}-x_{\\text {approx }}^{(i)}\\right\\|^{2}}{\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|x^{(i)}\\right\\|^{2}} \n",
    "%\n",
    "=\n",
    "%\n",
    "1 - \\frac{ \\sum_{i=1}^k S_{ii} }{ \\sum_{i=1}^n S_{ii} } \n",
    "\\leq 0.01 \\ \\ (1\\%) \n",
    "%\n",
    "\\end{align}\n",
    "\n",
    "**equivalently**\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{ \\sum_{i=1}^k S_{ii} }{ \\sum_{i=1}^n S_{ii} } \n",
    "\\geq 0.99 \\ \\ (99\\%)\n",
    "\\end{align}  \n",
    "\n",
    "Only need to call SVD once! Versus upstairs, you'd have to call this multiple times to update $k$ each time.  \n",
    "\n",
    "You should report the smallest value of $k$ that gives you what you need. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c65e3d-16c4-40b2-902f-7175fa9af49b",
   "metadata": {},
   "source": [
    "## Applying PCA: Advice for Applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a68636-f343-418e-9e1c-9b962fb90ff3",
   "metadata": {},
   "source": [
    "Supervised learning speedup:  \n",
    "\n",
    "Given original data set:  \n",
    "${(x^{(i)}, y^{(i)})}^m_1$  \n",
    "\n",
    "For $m=10^4$ we have a slow learning algorithm.  Instead reduce with PCA.  \n",
    "\n",
    "Extract inputs:\n",
    "- unlabeled dataset: $ x^{(i)} \\in R^{10000} \\ \\rightarrow \\ z^{(i)} \\in R^{1000}$\n",
    "- new training set: ${(z^{(i)}, y^{(i)})}^m_1$   \n",
    "\n",
    "Feed the reduced training data set to the learning algo.  \n",
    "\n",
    "For ex: we would train a hypothesis $h_\\theta (z)$ using $z^{(i)}$:\n",
    "\n",
    "$$x \\ \\rightarrow \\ z \\ \\rightarrow \\ h_\\theta(z) = \\frac{1}{1 + exp(-\\theta^T z)}$$\n",
    "\n",
    "Mapping PCA should be defined only by running the PCA on the training set:\n",
    "\n",
    "$$PCA: x^{(i)} \\ \\to \\ z^{(i)}$$. \n",
    "\n",
    "It's like saying \"PCA finds parameter $U_{reduce}$. \" So if we do this on the original training set, we can reuse it for the cross-validation set $x^{(i)}_{cv}$ and the text set $x^{(i)}_{test}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59deaea4-bc91-4091-b8d1-2abc6e8b35a6",
   "metadata": {},
   "source": [
    "### Bad Use Of PCA: To Prevent Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ed2bd-b65b-41af-9e28-6073fae7af4f",
   "metadata": {},
   "source": [
    "Idea: use $z^{(i)}$ instead of $x^{(i)}$ to reduce num features to $k$ from $n$. Therefore fewer features = less likely to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396741f-a4f4-418c-b217-8d7299eacd38",
   "metadata": {},
   "source": [
    "This is bad, because PCA throws away information! Regularization with penalty often works better, because each stage of the minimization still considers all of the parameters to be fitted.\n",
    "\n",
    "<img src=\"figures/fig21.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a23ec8-4553-46db-8f93-f5e3e651a6d9",
   "metadata": {},
   "source": [
    "### Bad Use Of PCA: Design Of ML System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015aedad-e14a-4f0b-a200-9dcd9eec6962",
   "metadata": {},
   "source": [
    "1. get train set: $(x^{(i)}, y^{(i)})$\n",
    "2. run PCA reduce $x^{(i)} \\to z^{(i)}$\n",
    "3. train logistic regression on $(z^{(i)}, y^{(i)})$\n",
    "4. test on test set:\n",
    "    1. map $x^{(i)}_{test} \\to  z^{(i)}_{test} $.\n",
    "    2. run $h_\\theta(z)$ on $(z^{(i)}, y^{(i)})$\n",
    "    \n",
    "INSTEAD: \n",
    "- always use the entire data set FIRST before using PCA. IE cutout step 2. unless you believe there is good reason to run PCA.\n",
    "- if you don't you will end up spending time on things like finding $k$ and figuring out this bs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6f8e5-1159-40ac-914a-67d3a8a575ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
