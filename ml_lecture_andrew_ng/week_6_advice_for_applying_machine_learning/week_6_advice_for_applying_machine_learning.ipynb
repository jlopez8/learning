{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520108db-47f7-4c48-be34-0b638e246926",
   "metadata": {},
   "source": [
    "# Week 6: Advice for Applying Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed9616-a1dc-4ede-8597-c418dee0ca01",
   "metadata": {},
   "source": [
    "What approaches / suggestions are good for what promising avenues are the most effective when deciding what ML tool to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dbc3f-ef62-433e-b00f-015aa6c32dbe",
   "metadata": {},
   "source": [
    "## Debug a learning algo:\n",
    "\n",
    "Suppose implemented lin reg to predict house prices:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\left[ \\sum_i^m (h_\\theta(x^{(i)} - y^{(i)})^2 + \\lambda \\sum_j^m \\theta_j^2 \\right]$$\n",
    "\n",
    "You find testing on new set of houses, there are large errors on predicitons.  \n",
    "\n",
    "**What to do next?**\n",
    "- get more training samples? \n",
    "    - :( sometimes this does not help!\n",
    "- try smaller set of features\n",
    "    - to prevent overfitting\n",
    "- try getting additional features\n",
    "- try adding polynomial featurs ($x_1 x_2$)\n",
    "- try increasing/decrease $\\lambda$\n",
    "\n",
    "**fortunately there is simple teqn to rule out half the list above** which saves a lot of time pursuiting something that will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f9a97-5ecc-4ea7-b21e-a70c87a0527e",
   "metadata": {},
   "source": [
    "## Machine Learning Diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bc0b0-52d3-4981-b79d-edc29fed2eb0",
   "metadata": {},
   "source": [
    "what is or is not working with an algo?\n",
    "\n",
    "What are promising things to try to improve performance?\n",
    "\n",
    "Can take time to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58272cfc-88c4-4303-bd73-0f5480277f02",
   "metadata": {},
   "source": [
    "## Evaluating A Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e14be6-0b7e-4669-bcea-fe3d3d084b71",
   "metadata": {},
   "source": [
    "Get low training error? \n",
    "- problem is overfitting\n",
    "\n",
    "How can you tell overfitting?\n",
    "- can plot for low dimensional problem / few features.\n",
    "- what about for many many features?\n",
    "    - split the data: training set (70%) + test set (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818a8cd-3fc6-47fe-9cb0-e5d8715901a1",
   "metadata": {},
   "source": [
    "### Training/Testing Process for Linear Reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4e26c-e597-47dd-81c2-05cedecf9e86",
   "metadata": {},
   "source": [
    "This is the standard teqn for evaluating how good learned hypothesis is.\n",
    "\n",
    "- learn $\\theta$ from training data (minimize $J(\\theta)$).\n",
    "- compute test set error: $J_{test}(\\theta) = \\frac{1}{2m_{test}} \\sum_i^{m_{test}}  h_\\theta(x^{(i)}_{test}) - y^{(i)}_{test}$\n",
    "- $ J_{\\text {test }}(\\theta)=-\\frac{1}{m_{\\text {test }}} \\sum_{i=1}^{m_{\\text {test }}} y_{\\text {test }}^{(i)} \\log h_{\\theta}\\left(x_{\\text {test }}^{(i)}\\right)+\\left(1-y_{\\text {test }}^{(i)}\\right) \\log h_{\\theta}\\left(x_{\\text {test }}^{(i)}\\right) $\n",
    "\n",
    "- Misclassification error:\n",
    "\\begin{align}\n",
    "    \\operatorname{err}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{array}{ll}\n",
    "1 & \\text { if } h_{0}(x) \\geqslant 0.5, \\quad y=0 \\\\\n",
    "& \\text { or if } h_{0}(x)<0.5, & y=1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\\right\\} \\text { error }\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text { Test error }=\\frac{1}{m_{test}} \\sum_{i=1}^{m_{\\text {test }}} \\operatorname{err}\\left(h_{\\theta}\\left(x_{\\text {test }}^{(i)}\\right), y^{(i)}\\right) \\text { . }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739589f-b01a-44cf-bdcc-3617358b7adf",
   "metadata": {},
   "source": [
    "## Model Selection and Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d912a6-3587-48dd-a5e8-d88dd7199ef2",
   "metadata": {},
   "source": [
    "Once params are fit to the training set (or similar), then error is measured on that data, which is likely to be lower than the general error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7ad7b-935a-4e83-95e8-76a64d15a308",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a8e4b-28ba-44cd-85d0-94df353352fb",
   "metadata": {},
   "source": [
    "Each model type will yield a different $\\theta$ fit. \n",
    "Each one has corresponding $J_{test}$. We can take the model w lowest $J_{test}$.\n",
    "\n",
    "<img src=\"figures/fig1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dc771-066d-4f78-bd46-d7efd9b8936f",
   "metadata": {},
   "source": [
    "**How well does this model generalize?**\n",
    "\n",
    "We can report $J_{test}(\\theta^{(d)})$.  \n",
    "Problem is likely to be an optimistic estimate of generalized error.\n",
    "\n",
    "Essentially, are fitting parameter $d$ to the test set, bc we are **choosing** the degree of the polynomial.  \n",
    "\n",
    "We fit the param $d$ to the test set, the performance of the hyp on the test set is likely to be misleading about the general performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54109678-b6b6-4111-ab3c-7a310ae0d33a",
   "metadata": {},
   "source": [
    "### How to Evaluate The Hypothesis:\n",
    "This is what do instead:  \n",
    "\n",
    "training set = 60%, cross-validation 20%, test set = 20%\n",
    "\n",
    "$\\Rightarrow$ $(x^{(i)}, y^{(i)} )$,   $(x_{cv}^{(i)}, y_{cv}^{(i)} )$,   $(x_{test}^{(i)}, y_{test}^{(i)} )$\n",
    "\n",
    "Produces three errors:  \n",
    "\n",
    "Training error:\n",
    "$$J_{train}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Cross Validation error:\n",
    "$$J_{train}(\\theta) = \\frac{1}{2m_{cv}} \\sum_{i=1}^m (h_{\\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$$\n",
    "\n",
    "Test error:\n",
    "$$J_{train}(\\theta) = \\frac{1}{2m_{test}} \\sum_{i=1}^m (h_{\\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f67fd9-bd31-4d0c-ae2a-3510505b882e",
   "metadata": {},
   "source": [
    "## Model Selection part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e067e1c-a6fb-4df1-ba08-4a666b4077f9",
   "metadata": {},
   "source": [
    "Instead of using test set to select model, instead use cross validation to select model. Do same steps / algo as before but only on cv data. Choose lowest $J_{cv}(\\theta^{(d)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a5ef8-735a-4c32-a228-610d0c0a1ade",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b43ac5-f7bd-4ad8-bf9f-dd2f8781a0e8",
   "metadata": {},
   "source": [
    "Now estimate generalization error for test set $J_{test}(\\theta^{(d)})$. This gives independence between the choice $d$ and the test/train data.  It is considered to have seperate train, validation, and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a45c19-03db-46df-ae10-0d0516590a03",
   "metadata": {},
   "source": [
    "## Diagnosing Bias vs. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bf760-83da-4ec2-9b2a-f13a78a20aba",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa9825-1013-47e4-a0ca-ee181191dd73",
   "metadata": {},
   "source": [
    "Suppose using polynomial error $d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e4d3e-7aa8-4db0-95b8-4f46ed170ba8",
   "metadata": {},
   "source": [
    "Plot error vs. $d$ and layer it with the cross validation error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b074837-a810-41ee-966b-35d816d37985",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig4.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b7bb7-3863-4c8d-938e-362798f946ae",
   "metadata": {},
   "source": [
    "Suppose we have learning algo, and it's performing not as well. Is it a bias problem or a varias problem?\n",
    "\n",
    "Look at the regions:    \n",
    "region 1 has hi bias. CV error is hi b/c we have an underfit, and the train error is also high for similar reasons. The model is truly not capturing enough of the relationships to make it work.\n",
    "\n",
    "region 2 has hi variance. CV error is hi b/c we have overfit to the data, so our model is not resilient to data changes. Train error is low because we have overfit to this data set. The model is overtrained to the training dat set.\n",
    "\n",
    "Bias (underfit):  \n",
    "- $J_{train}(\\theta)$ high\n",
    "- $J_{cv}(\\theta) \\approx J_{train}(\\theta)$ \n",
    "\n",
    "Variance (overfit):\n",
    "- $J_{train}(\\theta)$ low\n",
    "- $J_{cv}(\\theta) >> J_{train}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf92df-a981-4beb-90c4-1d9371690875",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b023b5f-f357-431d-9b1e-2af0bcac314b",
   "metadata": {},
   "source": [
    "## Regularization and Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9dad0-1bc4-49a7-9d46-92604160aef3",
   "metadata": {},
   "source": [
    "Large $\\lambda$ -> high bias (underfit). We punish having too many terms.  \n",
    "\n",
    "Intermediate $\\lambda$ -> good fit.  \n",
    "\n",
    "Small $\\lambda$ -> high variance (overfit). We do not eliminate any terms, leading potentially to an overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe12e1-6dfc-47e3-841b-263b4db9edcf",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig6.png\" width=600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e178895-118c-464b-bad7-eb1646a6e7c8",
   "metadata": {},
   "source": [
    "### Choosing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c9ff6-56d9-4951-9b62-7a4386f2f0bc",
   "metadata": {},
   "source": [
    "For this setting w regularization, we have these error functions as before.  \n",
    "\n",
    "We outline how auto-choose $\\lambda$ using these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d6ec1-8b7e-4053-a91a-fe43a6776dc8",
   "metadata": {},
   "source": [
    "\\begin{array}{l}\n",
    "h_{\\theta}(x)=\\theta_{0}+\\theta_{1} x+\\theta_{2} x^{2}+\\theta_{3} x^{3}+\\theta_{4} x^{4} \\\\[2ex]\n",
    "%\n",
    "%\n",
    "J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\frac{\\lambda}{2 m} \\sum_{j=1}^{m} \\theta_{j}^{2} \\\\[2ex]\n",
    "%\n",
    "%\n",
    "J_{t r a i n}(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} \\\\\n",
    "J_{c v}(\\theta)=\\frac{1}{2 m_{c v}} \\sum_{i=1}^{m_{c v}}\\left(h_{\\theta}\\left(x_{c v}^{(i)}\\right)-y_{c v}^{(i)}\\right)^{2} \\\\\n",
    "J_{\\text {test }}(\\theta)=\\frac{1}{2 m_{\\text {test }}} \\sum_{i=1}^{m_{\\text {test }}}\\left(h_{\\theta}\\left(x_{\\text {test }}^{(i)}\\right)-y_{\\text {test }}^{(i)}\\right)^{2}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578bed8c-676a-4559-871c-9bad9623c7f6",
   "metadata": {},
   "source": [
    "Given model:  \n",
    "$$h_\\theta = \\sum \\theta_i x^i$$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_i^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_j^m \\theta_j^2$$\n",
    "    \n",
    "Try $\\lambda=0$ and minimize $J(\\theta)$ then get $\\theta_1$.  \n",
    "\n",
    "Repeat with $\\lambda = 0.01$ and minimize $J(\\theta)$ then get $\\theta_2$.   \n",
    "\n",
    "Repeat ...\n",
    "\n",
    "**Then** Evaluate $J_{cv}(\\theta^{(\\lambda)})$ on the **cv** set and choose the minimum of these.\n",
    "\n",
    "**Test Error:** $J_{test}(\\theta^{(\\lambda)})$.\n",
    "\n",
    "This is similar to the previous model-selection scheme where this minimum choice awas based on $d$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb36b2a-1d2d-4ec1-b007-5c4629e2bf5d",
   "metadata": {},
   "source": [
    "We can plot how each $J(\\theta)$, it will look opposite that of how it is plot wrt $d$.\n",
    "\n",
    "As $\\lambda \\to \\infty$, we get increasing bias problem (underfit), because we are penalizing the number of basis functions.   \n",
    "\n",
    "As $\\lambda \\to 0$, we get increasing variance problem (overtrain/fit), because we are not removing enough basis functions.\n",
    "\n",
    "The ideal here is some middle point that's **just right**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14724179-8394-4fc5-a49f-e30727eb6e18",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig7.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a3775-970d-46b1-977e-cb75306f1069",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bde3c4-017e-427e-85ff-a69eee9c4613",
   "metadata": {},
   "source": [
    "Artificially reduce training set size (10, 20, 40 train examples), then plot training and cv errors on them.  \n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{array}{l}\n",
    "    J_{t r a i n}(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} \\\\[2.0ex]\n",
    "    J_{c v}(\\theta)=\\frac{1}{2 m_{c v}} \\sum_{i=1}^{m_{c v}}\\left(h_{\\theta}\\left(x_{c v}^{(i)}\\right)-y_{c v}^{(i)}\\right)^{2}\n",
    "    \\end{array}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8d3db-4d36-45cd-92c9-8b5807e99ad8",
   "metadata": {},
   "source": [
    "Watch what happens as we increase the number of training examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229cf0f-a87a-4813-8671-0ff0cc878ae6",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig8.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda276-a460-48f9-8612-aa7e11487a5c",
   "metadata": {},
   "source": [
    "Then we plot the training error on only the points used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6854f9-f5c2-416f-9007-382cee517278",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig9.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cbac7-28d2-4841-b132-f9001580fa27",
   "metadata": {},
   "source": [
    "### What if High Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32225ada-3073-4b9e-94a4-b2a987d87634",
   "metadata": {},
   "source": [
    "Hi bias means we are underfitting. Let's see what the CV error curves look like.  \n",
    "\n",
    "Even if we add more and more data, we will **plateau out** on the improvements made (diminish returns).  \n",
    "\n",
    "The cv and training errors will be close to each other if in the mode of high bias.  \n",
    "\n",
    "We can prevent wasting time of collecting more data by observing this is starting to happen.    \n",
    "\n",
    "Also has **high error**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d6fcc-18ca-4ac3-922f-4c058b726a3a",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig10.png\" width=400>\n",
    "\n",
    "<img src=\"figures/fig11.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e432ff-6e16-4ceb-ac23-3d18861acb9f",
   "metadata": {},
   "source": [
    "### What if High Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab947b5b-39f6-4008-827b-1536196f8a1e",
   "metadata": {},
   "source": [
    "What if we are overfitting/hi variance?  \n",
    "\n",
    "The training set error will be **low** because we ar overfitting to the data here.  The CV error will be farther away from the training error.\n",
    "\n",
    "**More data is likely to help** in a high variance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e0aec-9705-46c4-afe3-d208d53297be",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig12.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8567923-44c4-430f-9150-7bf16ba3d096",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig13.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08d104-52d6-49c1-9d67-e7b267502242",
   "metadata": {},
   "source": [
    "## Deciding What To Do Next Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aaeef1-0147-440e-8416-47501b4d4d61",
   "metadata": {},
   "source": [
    "If things dont' work as well as you'd like, what to do next:\n",
    "\n",
    "- get more training samples **high variance**\n",
    "- try smaller sets of features **high variance**\n",
    "- try getting additional features **high bias**\n",
    "- try adding polynomial features ($x_1x_2\\dots$) **high bias** \n",
    "- try decreasing $\\lambda$ **high bias**\n",
    "- try increasing $\\lambda$ **high variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cf2e4-a6d2-41c8-bbf1-556f922c89d0",
   "metadata": {},
   "source": [
    "## Overfitting Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f851d6-5f22-44ee-8394-2a01f3470d9b",
   "metadata": {},
   "source": [
    " Usually the larger, the better but the main disadvantage is computational \\$\\$\\$.  \n",
    " \n",
    "For larger networks, use $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678833e-ac52-4f07-97d1-9383b2b1f6c1",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig14.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de0e6b-fd4b-463e-ab6d-7baa058754c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
